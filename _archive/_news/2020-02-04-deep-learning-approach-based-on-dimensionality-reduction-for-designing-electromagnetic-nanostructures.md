---
category: news
title: "Deep learning approach based on dimensionality reduction for designing electromagnetic nanostructures"
excerpt: "In this paper, we demonstrate a computationally efficient new approach based on deep learning (DL) techniques for analysis, design and optimization of electromagnetic (EM) nanostructures. We use the strong correlation among features of a generic EM problem to considerably reduce the dimensionality of the problem and thus, the computational ..."
publishedDateTime: 2020-02-04T11:36:00Z
webUrl: "https://www.nature.com/articles/s41524-020-0276-y"
type: article
quality: 24
heat: -1
published: false

provider:
  name: Nature
  domain: nature.com

topics:
  - AI
  - Machine Learning

images:
  - url: "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41524-020-0276-y/MediaObjects/41524_2020_276_Fig1_HTML.png"
    width: 685
    height: 536
    title: "Deep learning approach based on dimensionality reduction for designing electromagnetic nanostructures"

related:
  - title: "Efficient Model Selection for Deep Neural Networks on Massively Parallel Processing Databases"
    excerpt: "In this session we will present an efficient way to train many deep learning model configurations at the same time with Greenplum, a free and open source massively parallel database based on PostgreSQL. The implementation involves distributing data to the workers that have GPUs available and hopping model state between those workers ..."
    publishedDateTime: 2020-02-04T15:28:00Z
    webUrl: "https://insidehpc.com/2020/02/efficient-model-selection-for-deep-neural-networks-on-massively-parallel-processing-databases/"
    type: article
    provider:
      name: insideHPC
      domain: insidehpc.com
    quality: 30
    images:
      - url: "https://insidehpc.com/wp-content/uploads/2020/02/frank.jpg"
        width: 300
        height: 300
  - title: "Google Open-Sources Reformer Efficient Deep-Learning Model"
    excerpt: "Researchers from Google AI recently open-sourced the Reformer, a more efficient version of the Transformer deep-learning model. Using a hashing trick for attention calculation and reversible residual layers, the Reformer can handle text sequences up to 1 million words while consuming only 16GB of memory on a single GPU accelerator. With such a ..."
    publishedDateTime: 2020-02-04T14:02:00Z
    webUrl: "https://www.infoq.com/news/2020/02/google-reformer-deep-learning/"
    type: article
    provider:
      name: InfoQ
      domain: infoq.com
    quality: 4

secured: "RTAaHJq8RCv6DqmIIRQf2Uglsx8AMM5p5+OSvcF8jxD8SZWdUyzwlJJ1VtPocDJsmMXR7cdEB82tThJ26k6t8ecR8G79/6Qws4EkmuHSHob481JMEGJV7BTiIaPbrPfjnx55X9T/r70jTn83hAAwhNzLBnsbTbBl+FhnVTdGu0M0Bi+rwdsaqPiYummCTJ7YhBeeAHG4dQJv95C4YunPjcYm7oHYMR0c0hwy+If/ccWK4WH7Y2SBUZmNSD5Um2FqZ2YaH55RX8hG9YzSoZEUKgAH5+Hs125GsGING6J2fjpUGJESyAx860q0mDNZ62qci86t5Ao+dHsqNMqeWImU1TcRe6MlQhY2q4jQ5pHMB5FtgnA+Fn/rNH/+W4z8H8NdoNrDI9/GgCmFsmRvePd85Bo/Bh3Cg7LXb5gWRcqTNT5YnoFTCfp6zNXnqwnWtpGegGutVbFjxoRGqckrBIKY3OfCoyc1f81sT/4sfyWXPTQ=;ajicjFiamdQupGmnZvFULg=="
---

