---
category: news
title: "Microsoft’s FocalNets Replace ViTs’ Self-Attention With Focal Modulation to Improve Visual Modelling"
excerpt: "In the five years since their introduction, transformer architectures have come to dominate the natural language processing research field. Recently, vision transformers (ViT) have also demonstrated their power and potential across a wide range of computer vision tasks."
publishedDateTime: 2022-03-25T13:52:00Z
originalUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/"
webUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/"
ampWebUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/amp/"
type: article
quality: 46
heat: 46
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI
  - Microsoft AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2022/03/image-88.png?resize=790%2C313&ssl=1"
    width: 790
    height: 313
    isCached: true

secured: "8uMtO4z25WTO6wNzmLbmwgZ7U17w2uLvIZWjBsEnGGkFtZecxoaE47knNTzDpDJKeoaUTp0A5aCBmkl/i7omKJpQvI+zXesYipe1QR7UDz4lY2/7a0BJjfJjDesYIF63YbFOf9Q9l9BtMNFYZbnc1K62bvSpUX8XQrXyFtIUeVUHZwlUGpm3fjfXqAglmQ+TNkzZ6iguphnjOo7X3ULyx3VrnPr30HZIDZ5GrmTkSrSB5gX0BTxR8VFeh3JX2VY2d7V+rsFSO4rK9g4ZKk73YKwUfLixBhO17IYMna7rNnXYDMZ3bKua0YXd7okqGT8X5BgvJE6Cce+aHGtqercrg3Ey0qr+y6od4ZLyEgQPvKN23yGzpwHfNpW0gCdc159lyFTugfWLryuyZFP542puhAsf/+cEaBagUQjmGMIU6y5jTQvZgCUcfJ2y7Kb0D/jS7RUT5q2tYLh5acHJPvrg48B32iKMUSQLbXYjQQrybkiOndAGfh7WRC9YoDR+QYXVcn8N6higrNkYYpnJuxTW1Q==;reMoC2dCJOlBRER3Ju0x/g=="
---

