---
category: news
title: "ChatGPT Safety Controls Can Be Easily Jailbroken"
excerpt: "New research serves as an urgent and compelling wake-up call to the artificial intelligence community, highlighting the essential need to prioritize creating and implementing comprehensive safety safeguards. The guardrails for widely used chatbots can be jailbroken, and the release of open-source code may make the task easier for hackers."
publishedDateTime: 2023-07-27T13:39:00Z
originalUrl: "https://wallstreetrebel.com/wsr/articles/chatgpt_safety_controls_can_be_easily_jailbroken/2023-07-27-09-39-46.html"
webUrl: "https://wallstreetrebel.com/wsr/articles/chatgpt_safety_controls_can_be_easily_jailbroken/2023-07-27-09-39-46.html"
type: article
quality: 39
heat: 39
published: false

provider:
  name: https//wallstreetrebel.com
  domain: wallstreetrebel.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://wallstreetrebel.com/wsr/images/design3/wallstreetrebel.png"
    width: 300
    height: 204
    isCached: true

secured: "IpDcr4J7y2Cm90ayAc8XuAT4sfo7GSJiaiyGVof9hNkSOIwbeI4PE4DmWBjuVPzMmcS+12QheOk97zdL0oLIQuXUmN4dY7OU5Q60CU0rXsM8uzunYT4FfJeQImQEtn9O9sesSly045fQEUiFDtE3RSkvEouJJTbY/UBzr9lLCXqr8755Ys4o1xshccSUtT+hRNsJeruE+A0QXrVizN5FqlI1LngamypnsEoxOb9huTh2AHE9wxFi7o8LSMbI/Z9gsKlr8tUWTCfb/arhtqM210iiOFeL6MfWqWtiGsYnllosZcCyyb0HtSbdgIxlhvEL9kEsCg8TFi84hC+NPZsvGI1ua5sakb+tBuJrrKTRzFI=;5qQRhG1I3gb+RWWeKs8N7g=="
---

