---
category: news
title: "‘MrsFormer’ Employs a Novel Multiresolution-Head Attention Mechanism to Cut Transformers’ Compute and Memory Costs"
excerpt: "Transformer architectures have demonstrated impressive performance improvements since their introduction in 2017 and are now the standard in the natural language processing and computer vision research fields."
publishedDateTime: 2022-11-15T17:36:00Z
originalUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/"
webUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/"
ampWebUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/amp/"
type: article

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2021/01/image-122.png?resize=790%2C320&ssl=1"
    width: 790
    height: 320
    isCached: true

secured: "q0RAJ0glUMVRXPB/7tJYmeV1QlRnCTi4u77Qd093UpS8nzDitYlIKBnLKDmRkgw5nd6UbpwVRQ5DA4xzbRG+dUOZ4efHD2CRCmmtAPbdYkxc7PwYuEWAaB2jtpFaoBXt0EpTPv3EXb8a/pqIKaTlLz56uPFFsg+XAxHEtWQeNA3KWJzpBAKn5k8KUsKzosx7Xl7bcMzrMmHGwths7MRd1OQfy4RpnGyKGF+0sCdyE6ZTjTSSBgDs2caA3Ai6hp6+eOxaiRoF/E7D7+s/5oczlGfK6b6aPxFTWZEnvU3n8IjO6tu/LPvCD7Vmy/wV0le0vIIXEeEA66KZq727+Mz+Run8HnzPo/oj6KdkGigxigJr4yyAoyARfEMpKNLN76Tec0nEoAz0RNJ3b5ON3HHu/B6rOxj/XAAVTsDRvS8fbufyiDqzHxqTjWp+eKXNN4g4HZkNcSR94dHhxhDxwPaC08IayL31mcEdRwdkiu0xpDHLSiUl/Oeb+mn1PboLutq4qkXWYoKsAsBOSjkP08Ps3A==;BeG4t6kJzgyI9F80SggIjg=="
---

