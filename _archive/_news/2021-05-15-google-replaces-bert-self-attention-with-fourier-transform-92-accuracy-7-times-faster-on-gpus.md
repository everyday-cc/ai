---
category: news
title: "Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs"
excerpt: "Transformer architectures have come to dominate the natural language processing (NLP) field since their 2017 introduction. One of the only limitations to transformer application is the huge computational overhead of its key component â€” a self-attention mechanism that scales with quadratic complexity with regard to sequence length."
publishedDateTime: 2021-05-14T21:47:00Z
originalUrl: "https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/"
webUrl: "https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/"
ampWebUrl: "https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/amp/"
type: article
quality: 31
heat: 31
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i2.wp.com/syncedreview.com/wp-content/uploads/2021/05/image-48.png?fit=785%2C429&ssl=1"
    width: 785
    height: 429
    isCached: true

secured: "R2n9h2ln6GEOLn8vZfAmap97E3cpqUZ+SASO8tJaJBUBXJ7e/7/d+1KwBALIY8xoB2u+pxykRZfFEprY8LCG5OLuLkTbV5WWRB2NAOIdYzlnBdv3s9hxDwi/wgyvpY6iULyTu0s1F1Kd1M9RyVlMezfQmbPzwYbnTCkg0eNLIyQVK7UOJ5EnzwT7HurEhqgT90sGRuRpo4ILN2Fg6viJ5Zu8xU5s8YVyn1ij8tVn86GHpqo4LzJKeOisQAO91odaAF3Gcg5a4nuSUWo8zGRAfmBH7wOm9K+WNa6MRUjIfDuvAASbp1OosXup8iDBYYmSbnpfLRwGq0V8Dz4PLOtJM2OXpFOTL29DEjGS5DJ9z8GAJ+RapSA0yrRi/Z3nLYsgUFXtaRdRHx6/4MAdGfpTX1pktxgpsaR1s+SIz28NI5tDqU0tQ0XN/ddaisQYdTnTEgu6IwbOMTXfjfdmp+wlA/E8B8z8udrRTZ73tSFO5hxYxKcO98yQmlQ6oQGCUEz4QRJrAGiggQBBiuXcjpyj7Q==;sNFd+diiQ6sPOX8Qy2rpjA=="
---

