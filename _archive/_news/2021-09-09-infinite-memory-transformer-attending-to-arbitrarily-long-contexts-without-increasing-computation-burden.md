---
category: news
title: "Infinite Memory Transformer: Attending to Arbitrarily Long Contexts Without Increasing Computation Burden"
excerpt: "When reading a novel, humans naturally remember relevant plot information even if it was presented many chapters earlier. Although todayâ€™s transformer-based language models have made impressive progress in natural language processing,"
publishedDateTime: 2021-09-09T15:43:00Z
originalUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/"
webUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/"
ampWebUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/amp/"
type: article
quality: 27
heat: 27
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Google AI
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2021/09/image-37.png?resize=582%2C650&ssl=1"
    width: 582
    height: 650
    isCached: true

secured: "ofDkwUXjCo/ImYBRgLN4ULJm7n/8ap/9tyBcsRzROMeiUt79g+0VuWYtbc4QbuglpwwqfCdksnl/irUlEFLfArHUi0vLJK3hBiVkXyfVCN8Cz7cL+Q2xcuFKnI8DzTP2NfhvEDjDjCwZ9inWjlKSz3Q6hD5IYjnTPLpF1PYtDWQLQwYNgCsgY4cm0Zu0Z35QPEjiRlpWm02buOHXijCx1/GhV70LHjxsYEKCLuHeyx8/X3d9rQVD85JGqfx0br+/2BuLT/I/yfYaKHuxMJaz4SSs77IyaZOsPbSWOXXvozIPbTHwGVMksr4GnuAkNud8vYkRJq1QZrZWNcaCxSIn76M4Zvc0VMaMqdHdmVLXukioDePssa66GywCFfYZwbQZ1pAZ1wUj1cotVo9Xw4eGtQRK1BDcjlFDT22knWjFWqUenyvY5reIB8U1h9vfAhwZ4FDrNyGBS5X5nKEJDQfjNQ5F/QRn2O78W9cIc2+F9iQ/P2/IL563U+CwW86u8CmaeIw6w4kiXIIdICbtHG55qw==;/CQV0vswsVez7E0mOiILGg=="
---

