---
category: news
title: "Hyperparameter optimization for fine-tuning pre-trained transformer models from Hugging Face"
excerpt: "Large attention-based transformer models have obtained massive gains on natural language processing (NLP). However, training these gigantic networks from scratch requires a tremendous amount of data and compute. For smaller NLP datasets, a simple yet effective strategy is to use a pre-trained transformer,"
publishedDateTime: 2022-06-29T16:57:30Z
originalUrl: "https://aws.amazon.com/blogs/machine-learning/hyperparameter-optimization-for-fine-tuning-pre-trained-transformer-models-from-hugging-face/"
webUrl: "https://aws.amazon.com/blogs/machine-learning/hyperparameter-optimization-for-fine-tuning-pre-trained-transformer-models-from-hugging-face/"
type: article
quality: 78
heat: 78
published: true

provider:
  name: AWS
  domain: aws.amazon.com
  images:
    - url: "https://everyday-cc.github.io/ai/assets/images/organizations/aws.amazon.com-50x50.jpg"
      width: 50
      height: 50

topics:
  - AI
  - AWS AI

images:
  - url: "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/06/29/Featured-images-for-ML-9045.jpg"
    width: 800
    height: 400
    isCached: true

secured: "1eIsAJjHq4UGtkUrcFITcI/63dev7mbjzxskIlHEGsdL1eM6zSHFPuvWZ+q0CU8sasLSm3AKlOLWHGhLmRcA7y2rUCgOmKeFSGsiBJbjFZGiIUjTNSgSoy8vqfD1YFOdCitHCjQbxwrq/onSTjRJg6tP8yfRwx8nq+W8rV45eavm6M9sT7IWtYf4tNs4Ni+ffH5LgAquS8WIvqe2Gvi8n/bbT1mIf0m/C3D0d7cdEPIuzm/TzQmG1yft/HhscUFpyqi1SvQYZa89xWcXUnYgD9JB8lUJKaqlRNdoYjk0IVTmOoL8+I1lH2jOsFyWjUGeZDHjmC8y55toZly2f+oB5VenTKcrtZJNium4Y4XhLV4=;GsDt4Qlzg9+GAi44h3RacA=="
---

