---
category: news
title: "Mistral AI's Open-Source Mixtral 8x7B Outperforms GPT-3.5"
excerpt: "Mistral AI recently released Mixtral 8x7B, a sparse mixture of experts (SMoE) large language model (LLM). The model contains 46.7B total parameters, but performs inference at the same speed and cost as models one-third that size."
publishedDateTime: 2024-01-23T16:58:00Z
originalUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
webUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
type: article
quality: 39
heat: 39
published: false

provider:
  name: InfoQ
  domain: infoq.com

topics:
  - Meta AI
  - AI

images:
  - url: "https://res.infoq.com/news/2024/01/mistral-ai-mixtral/en/headerimage/generatedHeaderImage-1705358046257.jpg"
    width: 1200
    height: 630
    isCached: true

secured: "KYiU+LYtUhPZWySQa4v1q9Eg8Cj5mreoQqouavtNOYtt04Ghq9o2KTXFGnXGQoetHeneM42tJkwwm6yCe9C9DPyv6e9PaunB0n2uYG28QQbc93yDE29FxqWwYCU3Y04ZRXGRryAceyGDZQEG4VHR1kg4OE2yrPkaTyRORfcn+LwX+CTKXJyYv4xdFVVvJTG5PDJEOMXDVsfeTqvC+clc1mWvACMh8jLTzGDFLphxzHtoJvVBjldl/3r31FAfnbOpSM7AwK7qKwIcWcMZTKGTqEb7MlH8q24EBR8hDiIGtXfOLkcr9WwooEZK2eADT0Gc+P8UKWsclir/4s63SFxFD4L63g/TqI+nzjL0dopqsK8LDBLYhuvkEmfl7ZPFpQ/3vRt4bBFGClTwHts5uxrW5zJ5bIZZtYKLV1lIyDkMLEnCZUSy0EXbdM1uu38x+mOL5oYokzJBxG+mQ4ZUHeC+v//hVnjB3S4rIv+5wgV75J5js2TOOtOEz9DuYnGcIVirmqV8eLv5NTIxu4D351vapw==;VZWkb+fI8LWNn8angcnRmw=="
---

