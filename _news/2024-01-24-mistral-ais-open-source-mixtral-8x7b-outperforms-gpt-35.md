---
category: news
title: "Mistral AI's Open-Source Mixtral 8x7B Outperforms GPT-3.5"
excerpt: "Mistral AI recently released Mixtral 8x7B, a sparse mixture of experts (SMoE) large language model (LLM). The model contains 46.7B total parameters, but performs inference at the same speed and cost as models one-third that size."
publishedDateTime: 2024-01-23T16:58:00Z
originalUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
webUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
type: article
quality: 19
heat: 19
published: false

provider:
  name: InfoQ
  domain: infoq.com

topics:
  - Meta AI
  - AI

secured: "PW5yifWI6ooizsC9z3ebWrc+r0ArLk8WW6fkbiawMWUb2MqZ8bT6dWvfTkaLa/u6OzAVmtPHf/xKHdLBQuletm/nRrfSaiUNXut1FmFDTAKpKDjEhyeakNkrIBpYVJ5fGRq1amKWSR71NSkxj9PbNSPHadQQmTAtP5KvTQK+ep7CjNzSC1rmuWcgj+pu/YF83uTSzFh//qryeaNczSCdFfG1iW5YdJTSCy1Wqx3vhhdGIN6P9AODYF20nqPxOysMPHomn88LTCVazUCNL0XPM3uLiGSgzsesKHu8Gp08sxteDtxy+Emy+LX606FA9fSWoLsZBWKBcEzlLS8ZK14m9/18hNT5ZFqrrYK68VNM9Qw=;Wavrjnxd/PMxMuUDWJBMOw=="
---

