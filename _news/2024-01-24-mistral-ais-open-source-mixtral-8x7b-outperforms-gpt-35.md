---
category: news
title: "Mistral AI's Open-Source Mixtral 8x7B Outperforms GPT-3.5"
excerpt: "Mistral AI recently released Mixtral 8x7B, a sparse mixture of experts (SMoE) large language model (LLM). The model contains 46.7B total parameters, but performs inference at the same speed and cost as models one-third that size."
publishedDateTime: 2024-01-23T16:58:00Z
originalUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
webUrl: "https://www.infoq.com/news/2024/01/mistral-ai-mixtral/"
type: article
quality: 39
heat: 39
published: false

provider:
  name: InfoQ
  domain: infoq.com

topics:
  - Meta AI
  - AI

images:
  - url: "https://res.infoq.com/news/2024/01/mistral-ai-mixtral/en/headerimage/generatedHeaderImage-1705358046257.jpg"
    width: 1200
    height: 630
    isCached: true

secured: "ZwyDV3cAIScuxyFV7kr8YInfFHPIJMSc99UZXCZUf11VKT1Alzwbp6oIwxyhAY6zAbCxQ0ppcyIsHPZNBjIyizJw0NZrVXe6wvzGxmgOhyMj2sIzEwQyCunOKABuI4d0XtAsVP8yK2wYbsIPS6l40oXAOabnULQc+F+ABgoL7OyTl9eLKR8l9vGhxb+aGlFPxeA18LTaVZqu9Fx5+iwaxjCiCAjQTKLSO7IMCUYbfKpz2n2v/3lzrZT+iD9uBYeKES9sJHy6kG2m4xtxoNluOEVCeiyijZohaC3fKv2Q2TLlV9A+v6orEoIPGmZ2uqt30TqkNkt+PnW+QYe+BzYMAVCCeUkIJC3wc0+5kJl4Fp8=;2r4aTuX6VG2gW9u+B+NCBw=="
---

