---
category: news
title: "Facebook AIâ€™s NormFormer Employs Extra Normalization to Significantly Improve Transformer Pretraining"
excerpt: "In a paper currently under review for ICLR 2022, a Facebook AI Research team introduces NormFormer, a modification to the Pre-LN transformer architecture designed to improve pretraining perplexity and downstream task performance for both causal and masked language models with negligible extra compute cost."
publishedDateTime: 2021-10-26T14:41:00Z
originalUrl: "https://syncedreview.com/2021/10/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-131/"
webUrl: "https://syncedreview.com/2021/10/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-131/"
ampWebUrl: "https://syncedreview.com/2021/10/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-131/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2021/10/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-131/amp/"
type: article
quality: 44
heat: 44
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Facebook AI
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2021/10/image-78.png?fit=950%2C534&ssl=1"
    width: 949
    height: 534
    isCached: true

secured: "yn69qK9+RSDB1JuDO45usSHH6fQoe9Ljvfb8lDX9yyx39bEBqKOlYlyg1i5SK/cDd+GP0n5vHfn5PD4OSokFOF7dGkP8u2++Lvpr47LcYoKIaxVMTK/kNhu76w+v970r/6yzW+F9ygL+qO0zrtJhCtIZyvSVtrtD9CppyKPHxrfxdrv5ZCf0Vzztr4dmU0cULn9CPehkzna326MNZHIAwdZaFTtkDw6x0HQgwYR0qSU1aNQM4p/P+wcueZpYZpw35i0NoQnsJ3Z1Ii2FlFCR7bI4HkBdLfJduqUMF73yKPXQXLEKOtsSRUhDuHBbAqDzNrR2FTJEpuvuxIH/Jvf78V6PeG7VpFvfzwCwRL7fsZ/6xJCL6P1VZs3qP95sz55QD8bUBQ8wmOwL+hgQ/vPHCTmu35X7Jv+eMCLxkLOf9+ZgyElaCYdYGfJ5cHnB5GkRiI5O6dm5RQw8sfEazEkedjy5QaesfoW2l29GiaYx07EpjNx8198x3Cj7g18XoMK7169ejFoi9ucgdeQdbmnpUA==;UoX1ulMAEMrPvvBn+b5r7w=="
---

