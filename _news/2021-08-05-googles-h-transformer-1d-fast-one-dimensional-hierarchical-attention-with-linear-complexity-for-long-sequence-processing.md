---
category: news
title: "Google’s H-Transformer-1D: Fast One-Dimensional Hierarchical Attention With Linear Complexity for Long Sequence Processing"
excerpt: "Transformer architectures’ powerful attention mechanisms are capable of pushing SOTA performance across various natural language processing (NLP) tasks. The quadratic complexity of run time and memory usage for such attention mechanisms however has long been a critical bottleneck when processing long sequences."
publishedDateTime: 2021-08-05T15:02:00Z
originalUrl: "https://syncedreview.com/2021/08/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-76/"
webUrl: "https://syncedreview.com/2021/08/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-76/"
ampWebUrl: "https://syncedreview.com/2021/08/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-76/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2021/08/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-76/amp/"
type: article
quality: 18
heat: 18
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i2.wp.com/syncedreview.com/wp-content/uploads/2021/08/image-14.png?fit=800%2C533&ssl=1"
    width: 800
    height: 533
    isCached: true

secured: "a06+0G1xsg/gYHP8HDs6BfJ1EyOvcmqF0WHYBNyMhNIRB/M5lvoZ9SWnK7RnyI3/qjHa4OjJsuT/Z9Y9fWWcAAlJakUrkzl03frDW1c9ioCWhVUGBfSwzew7KgEovj0JddcLPw5zRi5QUQcMkWLgUt+ttWBPUyGCA8M0QHnTmeDXBUxxQXMUghhX50GojRhEYmebw3kxgFqliz5kGS/AG3hgMK7x68uqfDUdEECaaVG9UbeW17LpUhzQQcgTKtVnQzSwNN+5HrdBjowBpx3GppW2FK/bqShKLIXKWRk8T1WZGAzSEMZxvaKnV2vwWl21T9Xf1h875v4C3GTs36Vcy5/WbQNlMMgfUfrMtm7/NoEeabKGvFOcisRS8ocyIxzwJBnnAclQwFyKFQKvPvsXHyZrT1Ie2GKa4qF/UeWPLwz0iK+vnH0NFZzzMf4QmqVFBf9S9TShkkaHXuaAqudV7oo/0GS7b/5N6z8QKZVEhETOXu6EAJfhhlSa+H7Vr4uP53yt77HlwN9uJGuwGrgzVw==;esKzxu/sFxmsM+uDWvwZRQ=="
---

