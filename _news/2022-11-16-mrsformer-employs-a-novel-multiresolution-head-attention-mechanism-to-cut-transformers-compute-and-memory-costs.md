---
category: news
title: "‘MrsFormer’ Employs a Novel Multiresolution-Head Attention Mechanism to Cut Transformers’ Compute and Memory Costs"
excerpt: "Transformer architectures have demonstrated impressive performance improvements since their introduction in 2017 and are now the standard in the natural language processing and computer vision research fields."
publishedDateTime: 2022-11-15T17:36:00Z
originalUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/"
webUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/"
ampWebUrl: "https://syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/11/14/mrsformer-employs-a-nove-multiresolution-head-attention-mechanism-to-cut-transformers-compute-and-memory-costs/amp/"
type: article

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2021/01/image-122.png?resize=790%2C320&ssl=1"
    width: 790
    height: 320
    isCached: true

secured: "+2/MwauGQvO/lU14p/a+ZU2JwZgi6R3vZ6LvT3eYYcxS+9BfXd2Chaq3VUWxUOfQK7G392Qc+PrFWWM0DtjhVqfnEy2+hx4sK9mH5GAEwf9xcuzLfDd9HpbZCtZr++DpTymscw2ojVUViwDacQf3LzwrrLyDDJUU9Q1CQJbbR0Mr85OfFwPIk1BflvB7P5HD2fooLGVaqVebWvbSF18HdV0A1PdRDXPYsLDygcPJS2/R9h5swFfZ1Vu0xW7DWuRtcnQ/TfzNuXQ/UoqogiO4YpX5K5HQnZ2dRUpo+ULGeL2VnEjtE/ueUHfNj93P0URJWSI4Vl2Te/fF6AeiettKDb9KYkO7jNKmqm+Ua1vtv0M=;4wdiiKHNYieYQL3osFKGig=="
---

