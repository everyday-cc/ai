---
category: news
title: "Infinite Memory Transformer: Attending to Arbitrarily Long Contexts Without Increasing Computation Burden"
excerpt: "When reading a novel, humans naturally remember relevant plot information even if it was presented many chapters earlier. Although todayâ€™s transformer-based language models have made impressive progress in natural language processing,"
publishedDateTime: 2021-09-09T15:43:00Z
originalUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/"
webUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/"
ampWebUrl: "https://syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2021/09/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-100/amp/"
type: article
quality: 27
heat: 27
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Google AI
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2021/09/image-37.png?resize=582%2C650&ssl=1"
    width: 582
    height: 650
    isCached: true

secured: "46uudPseIK0sdhGCh7Y8TvUKsagN8irEZQv95ro3tro9PM6cGd2dQym2EUBPkDi8x5RjVBk8uWdmnV1vEhKeveMPWbPipXlTjgoiAYp5bKqkVa7VSkVevh1f1zoiYJy9a0XWGB/0gU9DOiGIU44nCRZu/+Ut9qNrlmIwzSJ4t//ZyUY1NupJypy2is5lFBo2CoZiOrd+m24to10sCbVjSCxYxXZeyyiB648zzz4kdv8tGF1gdUyzu7hcIR3rOxgxY3+zKdrxP7T6HuGCbocA7wuqku9uhE6WmYUfW6V9cesqkr27JukC1/j/QCU/3N/HXLSPmTNrNdWRFUvFjZpsAHVcV0A54vL0PNLEtE+bTeU=;pJ116f/W6gyL30330d2EeQ=="
---

