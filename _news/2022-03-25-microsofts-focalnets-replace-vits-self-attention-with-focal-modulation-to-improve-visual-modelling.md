---
category: news
title: "Microsoft’s FocalNets Replace ViTs’ Self-Attention With Focal Modulation to Improve Visual Modelling"
excerpt: "In the five years since their introduction, transformer architectures have come to dominate the natural language processing research field. Recently, vision transformers (ViT) have also demonstrated their power and potential across a wide range of computer vision tasks."
publishedDateTime: 2022-03-25T13:52:00Z
originalUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/"
webUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/"
ampWebUrl: "https://syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/03/25/microsofts-focalnets-replace-vits-self-attention-with-focal-modulation-to-improve-visual-modelling/amp/"
type: article
quality: 46
heat: 46
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI
  - Microsoft AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2022/03/image-88.png?resize=790%2C313&ssl=1"
    width: 790
    height: 313
    isCached: true

secured: "4fmJ9sNsffnU/2Vdv8siwJWzZfZHSVI+XqAKySX1b+f0iLS5XpuRZ506eeqDWQ+YRZa1to7VKVjJstGdmOgLaIleJLt/6fR5jy9LnuqbSs10ENH3tUbmJWW9vYgn+sJjwpu8K+4wSpfcr39CivKITp8SbxRC+hJRy+epfk3WFB/v0UrZMB9QQA7elmngdaHt7RMVZAjqZQyaRkNhbjtI3Fu2pHVMcOi9ISc7YyV6guCDKfj814oJdvpjYlNXtPciobjbmIpeprIj6Rna/XlDY3zWlf+rnOrZ1sWMSXv9IY5tFkAenTz4Rk0C16D4kEfS7eoyiUoBsPjZ7JB5+DP8o0CMXv+oO4BwkMgRpRycm70=;j/RybNHbqOD1g4xKPy1C3g=="
---

