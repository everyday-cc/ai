---
category: news
title: "Mixtral-8x7B is now available in Amazon SageMaker JumpStart"
excerpt: "Today, we are excited to announce that the Mixtral-8x7B large language model (LLM), developed by Mistral AI, is available for customers through Amazon SageMaker JumpStart to deploy with one click for running inference. The Mixtral-8x7B LLM is a pre-trained sparse mixture of expert model, based on a 7-billion"
publishedDateTime: 2023-12-22T17:15:59Z
originalUrl: "https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/"
webUrl: "https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/"
type: article
quality: 104
heat: 124
published: true

provider:
  name: AWS
  domain: aws.amazon.com
  images:
    - url: "https://everyday-cc.github.io/ai/assets/images/organizations/aws.amazon.com-50x50.jpg"
      width: 50
      height: 50

topics:
  - AI
  - AWS AI

images:
  - url: "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/12/22/mistral-sagemaker-jumpstart.jpg"
    width: 1005
    height: 504
    isCached: true

related:
  - title: "Amazon SageMaker model parallel library now accelerates PyTorch FSDP workloads by up to 20%"
    excerpt: "Large language model (LLM) training has surged in popularity over the last year with the release of several popular models such as Llama 2, Falcon, and Mistral. Customers are now pre-training and fine-tuning LLMs ranging from 1 billion to over 175 billion parameters to optimize model performance for"
    publishedDateTime: 2023-12-22T22:04:14Z
    webUrl: "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-model-parallel-library-now-accelerates-pytorch-fsdp-workloads-by-up-to-20/"
    type: article
    provider:
      name: AWS
      domain: aws.amazon.com
    quality: 96
    images:
      - url: "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/12/22/sagemaker-model-parallel-library-pytorch.jpg"
        width: 1010
        height: 505
        isCached: true
  - title: "How to fine tune Mixtral 8x7B Mistral Ai Mixture of Experts (MoE) AI model"
    excerpt: "To begin the fine-tuning process, itâ€™s important to set up a robust GPU environment. A configuration with at least ... verify that the model is ready for deployment. Engaging with the AI community by sharing your progress and seeking feedback can provide ..."
    publishedDateTime: 2023-12-26T11:45:00Z
    webUrl: "https://www.geeky-gadgets.com/fine-tune-mixtral-8x7b/"
    type: article
    provider:
      name: Geeky Gadgets
      domain: geeky-gadgets.com
    quality: 37
    images:
      - url: "https://www.geeky-gadgets.com/wp-content/uploads/2023/12/How-to-fine-tune-Mixtral-8x7B.jpg"
        width: 1280
        height: 716
        isCached: true

secured: "udvOfGO7pt9zKr1MMaMmDC+aUnhP3qR8k084GYaMzhxj49ThfwPdR8DAW1gcAEPK6fTM37bGOPNoL0HMeWwpSPN8fXqGizMOYed5/3cHNpJUo9zfiF5sGYo8gFYizBdoNP5HsPVC2Uvp5OLfI3LVyTOxSwUiKFcpwjxVfHFdQYl7gWhtOmzCrqO4Okd+RrF9HykBYAQkmJNEP1qOD553LdhOm3/NuWJdsWXTsH/mtoA2ECVCTIMrL7O9LR21teMIe7Am8hAbccsVqh9jeBkxFlNrLvYWteKZSaYNDuU0jqTkeShXvf2GcSoLcGMMD5LDzbeuVnxSH2p7vVMfncwbSX3X6I0MlLOs1CbzgodFwlM=;+Kp2KooUi0G9Hj21w14/9A=="
---

