---
category: news
title: "Google, NYU & Maryland U’s Token-Dropping Approach Reduces BERT Pretraining Time by 25%"
excerpt: "The pretraining of BERT-type large language models — which can scale up to billions of parameters — is crucial for obtaining state-of-the-art performance on many natural language processing (NLP) tasks."
publishedDateTime: 2022-03-29T14:20:00Z
originalUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/"
webUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/"
ampWebUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/amp/"
type: article
quality: 51
heat: 51
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2022/03/image-97.png?resize=542%2C428&ssl=1"
    width: 542
    height: 428
    isCached: true

secured: "3nc/78uHAfVosTHA3JcwWgC6lPzPTVLvvaOVgPRa04oM+r51Afq03/WaXOA+oElrkIPwxrwVzJb2WByPH+tLDkxHBbT5NL0HUkf59rXW76zBtXjxtcKrLUfUzq47T4LOTznZ40KHSVICubbAa97X/tShi6Nbg7tRYgM+IsVT5QocilenyeW+Iq/Lk94hUy68L1TiFkU8uwGoiDThghjtN8Bql1nKfax4Q8ySsVitVP5q0oZvpPVYVWFouox7v1CRq+7tk/BQXmP38uGoZgYYR9+a/8qaMXKTbQuT/0FV78Bd0QFd8lEqij9CUHpbCQCuIPdCDb1V6gGuCuokj+HyNpk/4lBJ5vla2ceETlEgCg0=;xoPMBWLIug0c/7iMHIfrvA=="
---

