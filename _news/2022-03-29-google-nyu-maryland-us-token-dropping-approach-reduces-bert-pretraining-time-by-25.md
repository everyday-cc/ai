---
category: news
title: "Google, NYU & Maryland U’s Token-Dropping Approach Reduces BERT Pretraining Time by 25%"
excerpt: "The pretraining of BERT-type large language models — which can scale up to billions of parameters — is crucial for obtaining state-of-the-art performance on many natural language processing (NLP) tasks."
publishedDateTime: 2022-03-29T14:20:00Z
originalUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/"
webUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/"
ampWebUrl: "https://syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/amp/"
cdnAmpWebUrl: "https://syncedreview-com.cdn.ampproject.org/c/s/syncedreview.com/2022/03/29/google-nyu-maryland-us-token-dropping-approach-reduces-bert-pretraining-time-by-25/amp/"
type: article
quality: 51
heat: 51
published: false

provider:
  name: syncedreview
  domain: syncedreview.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://i0.wp.com/syncedreview.com/wp-content/uploads/2022/03/image-97.png?resize=542%2C428&ssl=1"
    width: 542
    height: 428
    isCached: true

secured: "xmYE8DAGFElc1TLyRrz53AuQpeV0FQz6fOVEjooWFox0SdOvKrjM9z2rigganWHqwVH8cH5y8FkPcbUmqxv+kcPD7DH05IhsYlWa0Ajq56bx5yMd/MU9/3DHFpAyD0MtRILvLClBSowcaubcbd/z50AUZ2G5GDoTqz1JfqI0nMRRCccMfPFHST20Sze1r+4I4MFIcj8mvF9Q6jGTR3Ldis0oxFiORbSZTzD9cL5a8dDDQp5WhSyG+/u5gK8yECYN/bMruFYnHYGhjyNmSsLeCMUOgGrlV5CxB5bYi2P52VJAr/nqrqPllYaUCde22m0YqxAIHGL6RzOLa4f0og23lA6QsPSknF/phwePPQ/LiJ5oiXtnyzH47R+xdtoq+YjNErlPbybgNKF7kAGJhcBpJ/nhJOmn/M+zH8GNlJvPsN41p54HYFSyxyqMYp6pin6EGQyPlMtxKKIwyoxx5Lo2BYJPU6XclA7GOdbEFpA+3GeZdS05APAefghFVJBo9i2YWdY+c3Oxn6zKvQFT1FKYfA==;5fAfrb9I4vLCvDWBIJsV8A=="
---

