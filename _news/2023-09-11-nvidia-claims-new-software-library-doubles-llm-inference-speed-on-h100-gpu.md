---
category: news
title: "Nvidia claims new software library doubles LLM inference speed on H100 GPU"
excerpt: "TensorRT-LLM will be integrated into Nvidia's NeMo LLM framework as part of the Nvidia AI Enterprise software suite early next ... batching' scheduler which allows work to enter and exit the GPU independent of other tasks. The library also offers automatic ..."
publishedDateTime: 2023-09-10T22:37:00Z
originalUrl: "https://www.datacenterdynamics.com/en/news/nvidia-claims-new-software-library-doubles-llm-inference-speed-on-h100-gpu/?n=@"
webUrl: "https://www.datacenterdynamics.com/en/news/nvidia-claims-new-software-library-doubles-llm-inference-speed-on-h100-gpu/?n=@"
type: article
quality: 52
heat: 72
published: true

provider:
  name: Datacenter Dynamics
  domain: datacenterdynamics.com

topics:
  - AI Hardware
  - AI

images:
  - url: "https://media.datacenterdynamics.com/media/images/NvidiaLogo.2e16d0ba.fill-1200x630.jpg"
    width: 1200
    height: 630
    isCached: true

related:
  - title: "How NVIDIAâ€™s TensorRT-LLM is making AI and LLMs more accessible"
    excerpt: "NVIDIA's TensorRT-LLM streamlines the deployment of LLMs, enhancing their performance and accessibility for AI applications."
    publishedDateTime: 2023-09-12T01:25:00Z
    webUrl: "https://techwireasia.com/2023/09/can-tensorrt-llm-unlock-the-full-potential-of-llms-ai/"
    type: article
    provider:
      name: techwireasia.com
      domain: techwireasia.com
    quality: 39
    images:
      - url: "https://techwireasia.com/wp-content/uploads/2023/09/11092023_NVIDIAs-solution-for-scalable-AI-and-LLMs-e1694420342925.png"
        width: 1440
        height: 810
        isCached: true
  - title: "Nvidia Says New Software Will Double LLM Inference Speed On H100 GPU"
    excerpt: "Nvidia said it plans to release open-source software that will significantly speed up inference performance for large language models powered by its GPUs, including the H100."
    publishedDateTime: 2023-09-08T18:42:00Z
    webUrl: "https://www.crn.com/news/components-peripherals/nvidia-says-new-software-will-double-llm-inference-speed-on-h100-gpu"
    type: article
    provider:
      name: CRN
      domain: crn.com
    quality: 19

secured: "WGB4RmCzsxmbbZpBMKgXEXZjViVIews3hN+zrp8OvvdoBHMFcBpcvW9kLqxX0/sdVzQ7jxyhf4LmraMubeXi18rzLCY2n+/UJIVvwW0lXLSkYl07rmLoNkRsWZYDMM9L1yN+qo4OpwIryobLo/mEw8ohXhTrQ3Gu6YgWjOurF3oQNH1yvB3qB9ADTYXn1U/PRE0L3sJaX6Oq/hN+NwHuW27vzmmYbxtuDspYIi+kz+aM8arfZzim+I9l34Bi+jn9nFLoy1H4HHPxqEBOEAjQIyvKx+ClRyd7ziPgKIKDusKCqvgs2pOAY5kn/G4ai/iYxOhzKD8bImG3O2+aXwcGePzb++u4R4DGYozCp69CWAk=;BmeYbyd95q5/Pa7mR9Pf8g=="
---

