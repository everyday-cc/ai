---
category: news
title: "Benchmark and optimize endpoint deployment in Amazon SageMaker JumpStartÂ "
excerpt: "When deploying a large language model (LLM), machine learning (ML) practitioners typically care about two measurements for model serving performance: latency, defined by the time it takes to generate a single token, and throughput, defined by the number of tokens generated per second. Although a single"
publishedDateTime: 2024-01-29T18:34:21Z
originalUrl: "https://aws.amazon.com/blogs/machine-learning/benchmark-and-optimize-endpoint-deployment-in-amazon-sagemaker-jumpstart/"
webUrl: "https://aws.amazon.com/blogs/machine-learning/benchmark-and-optimize-endpoint-deployment-in-amazon-sagemaker-jumpstart/"
type: article
quality: 88
heat: 88
published: true

provider:
  name: AWS
  domain: aws.amazon.com
  images:
    - url: "https://everyday-cc.github.io/ai/assets/images/organizations/aws.amazon.com-50x50.jpg"
      width: 50
      height: 50

topics:
  - AI
  - AWS AI

images:
  - url: "https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/01/19/ML-15932-image002-1-1260x504.jpg"
    width: 1260
    height: 504
    isCached: true

secured: "6x0ZR1V4YezI6P8rmcfr6i7uMBsYQoLo7SQLiHcudB7kSfJo8A3XgOkrcvE1zhTOw7KhmRYAdHy4SV1dQ5jXLOXs1RuSgesyE8zHMFGTtVjzrgKXSUdm/+Oz6t+c9lnFiOxV/Jd0RQHWn1Ppbn4e9iYVHvzawdTTHPmH/z0RNAmvgRM90BHb79khv5jvrFUGAQto1YCumdumf6VB0Op1jnYWwAAIB2hexb5Ar54fqBGK7a16XCm4x2p7Eq/e3tBLBxRvBzmtAm5LFnVrk3BUKCq935ijsgbMs689MlR3RjFiGy+NheLULlvXNKq7mw+sHppRYexhW+ABAUnk+aYbODq4SF/AoY7sDzFYRvnIgnM=;jmDXaiItmADRm5AbpzB5ow=="
---

