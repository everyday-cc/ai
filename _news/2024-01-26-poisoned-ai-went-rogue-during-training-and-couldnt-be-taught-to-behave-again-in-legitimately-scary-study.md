---
category: news
title: "Poisoned AI went rogue during training and couldn't be taught to behave again in 'legitimately scary' study"
excerpt: "AI researchers found that widely used safety training techniques failed to remove malicious behavior from large language models â€” and one technique even backfired, teaching the AI to recognize its triggers and better hide its bad behavior from the researchers."
publishedDateTime: 2024-01-26T10:59:00Z
originalUrl: "https://www.livescience.com/technology/artificial-intelligence/legitimately-scary-anthropic-ai-poisoned-rogue-evil-couldnt-be-taught-how-to-behave-again"
webUrl: "https://www.livescience.com/technology/artificial-intelligence/legitimately-scary-anthropic-ai-poisoned-rogue-evil-couldnt-be-taught-how-to-behave-again"
type: article
quality: 19
heat: 19
published: false

provider:
  name: Live Science
  domain: livescience.com

topics:
  - AI

images:
  - url: "https://cdn.mos.cms.futurecdn.net/igZKQnzMUXuCcet2MwQ5bF-1200-80.jpg"
    width: 1200
    height: 675
    isCached: true

secured: "3ylnBSbyodfALJbU+JCyilgpDmca+mSEKxMfW/XTmtlzsAUOPWJNF1NONOWNeZSOls3eUbife0gwjVcOrj4TxQTYbbOn85SMIk553YYOHHitTeRkfxXkACjMzwfTltwgQVju3f95izHuMd3EqHHLkaSmQ1SxjG3f4Re1uCP3ErmMYZWkKRyop1EUXFJmf8mkoje5x1Zd7O/RSpCUitcm1KGL1y2tl7V3lud8ZVV0BOf0Zk//cnOuATaK1OpFftrf78m5xBm596PA69dmlTBXjdwKZ/E5u5iaUG0VzOX+MEkNclE3uhAn7Mu1wn9/HIHbytCdtxSqg9sYeIE/39oTUsf6L88c0Kj0m1WXm+cKyhY=;WYiXnUX8rulQOwFDlvRCCw=="
---

