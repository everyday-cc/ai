---
category: news
title: "Poisoned AI went rogue during training and couldn't be taught to behave again in 'legitimately scary' study"
excerpt: "AI researchers found that widely used safety training techniques failed to remove malicious behavior from large language models â€” and one technique even backfired, teaching the AI to recognize its triggers and better hide its bad behavior from the researchers."
publishedDateTime: 2024-01-26T10:59:00Z
originalUrl: "https://www.livescience.com/technology/artificial-intelligence/legitimately-scary-anthropic-ai-poisoned-rogue-evil-couldnt-be-taught-how-to-behave-again"
webUrl: "https://www.livescience.com/technology/artificial-intelligence/legitimately-scary-anthropic-ai-poisoned-rogue-evil-couldnt-be-taught-how-to-behave-again"
type: article
quality: 19
heat: 19
published: false

provider:
  name: Live Science
  domain: livescience.com

topics:
  - AI

images:
  - url: "https://cdn.mos.cms.futurecdn.net/igZKQnzMUXuCcet2MwQ5bF-1200-80.jpg"
    width: 1200
    height: 675
    isCached: true

secured: "oMuwOSQ+ZWHQtblPcg4JH6jFOmA8dXA+Mwk/dneUAvArDpiXti+6iZ9XPIL+coQa1QzunVnwQLBDMoSnQCsHCVzlWMJ7GgZALV9t53r1j/T/VTlxXUWOIRU5eFeHCwoZ+FHJYON8gkJKjNqV80LjLzVvaDWetdpIYGCe8Xip2ppDn7Pr9Z3WFR68cgCmHOXVtn/kvJ86KKszl+4yv/UbSaWBZgJQ6+JLy156T9vv+pFbaXa0wF4laiVKrUFM55sLsbhzofCmU2fO182OoK7nXuj9Jx16s7KtSqJPCvg7t7Xkyn/Wo8XdQkl7urp+4MwRF3Fy67THnVwoXjXkxlBMSOqeOdC1VIMHKcZo6mp1LpQCZhCehwXsUqgeCm40e90BQxpw4ABDHJg6SdktXgFO0BYe3Smy1HEoJN7qULEYWii+x5Q2bTSFqGQb5DcDle6ekGTtwweop1BaPpXAz+lqGcIW54M1Sq0vMtCHqDI9WMFAeiB32od7jtK4+cXNsMyd9eNFmSWEfU8Sk4QaDJFUbA==;Mzt+ev1hjEqfopvwA8xpuA=="
---

